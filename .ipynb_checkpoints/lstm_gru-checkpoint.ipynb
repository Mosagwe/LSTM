{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82552b89",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the libraries\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfivethirtyeight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Import entire .csv file as a DataFrame\n",
    "dataset = pd.read_csv('C:/data/ML_Models/RNN/IBM.csv',index_col='Date',parse_dates=['Date'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be7392b",
   "metadata": {},
   "source": [
    "#### Column selection and dataset slicing\n",
    "For this case \"Close\" stock price column is selected, which indicates IBM's closing price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting column 'Close'\n",
    "training_set = dataset[:'2016'].iloc[:,3:4].values\n",
    "test_set = dataset['2017':].iloc[:,3:4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189fb8b0",
   "metadata": {},
   "source": [
    "#### Apply Feature Scaling to the Data set\n",
    "Used normalization technique for feature scaling using MinMaxscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ed3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the training set\n",
    "sc = MinMaxScaler()\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38bfd0",
   "metadata": {},
   "source": [
    "##### 'Close' attribute for prices visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318a5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Close' attribute for prices visualization.\n",
    "dataset['Close'][:'2016'].plot(figsize=(16,4),legend=True)\n",
    "dataset['Close']['2017':].plot(figsize=(16,4),legend=True)\n",
    "plt.legend(['Training set','Test set'] )\n",
    "plt.title('IBM stock price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc9599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e680aba",
   "metadata": {},
   "source": [
    "#### Specify the Number of Timesteps\n",
    "Timesteps specify how many previous observations should be considered when the recurrent neural network makes a prediction about the current observation. \n",
    "In this case, 60 timesteps is used, menaing for every day the neural network predicts, it will consider the previous 60 days of stock prices to determine its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcaeafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So for each element of training set, we have 60 previous training set elements \n",
    "X_train_data = []\n",
    "y_train_data = []\n",
    "for i in range(60,2769):\n",
    "    # for i in range(60,len(training_set)):\n",
    "    X_train_data.append(training_set_scaled[i-60:i,0])\n",
    "    y_train_data.append(training_set_scaled[i,0])\n",
    "    \n",
    "# Transform to Numpy array\n",
    "X_train_data, y_train_data = np.array(X_train_data), np.array(y_train_data)\n",
    "\n",
    "# Reshaping X_train_data for efficient modelling\n",
    "# Reason for reshaping is that the recurrent neural network layer only accepts data in a specific format\n",
    "# Use np.reshape method\n",
    "X_train_data = np.reshape(X_train_data, \n",
    "                          (X_train_data.shape[0],\n",
    "                           X_train_data.shape[1],\n",
    "                           1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d049f30",
   "metadata": {},
   "source": [
    "This outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06397bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd4357",
   "metadata": {},
   "source": [
    "### Building Recurrent Neural Network\n",
    "### 1. Long Short Term Memory (LSTM)\n",
    "Used Sequential class i.e. add sequences of layers over time to the build recurrent neural network.\n",
    "##### Dropout Regularization\n",
    "Dropout regularization is a technique used to avoid overfitting when training neural networks.\n",
    "\n",
    "Epochs: the number of iterations for the recurrent neural network to be trained on. \n",
    "\n",
    "The batch size: the size of batches that the network will be trained in through each epoch.\n",
    "\n",
    "Used Adam optimizer which is a workhorse optimizer that is useful in a wide variety of neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize recurrent neural network\n",
    "rnn = Sequential()\n",
    "# Add more layers to this recurrent neural network using the add method\n",
    "# First LSTM layer with Dropout regularisation\n",
    "rnn.add(LSTM(units=50, return_sequences=True, input_shape=(X_train_data.shape[1],1)))\n",
    "# Adding Some Dropout Regularization\n",
    "# Dropout regularization is a technique used to avoid overfitting when training neural networks.\n",
    "rnn.add(Dropout(0.2))\n",
    "\n",
    "# Add Three more LSTM layer\n",
    "# 2nd LSTM layer\n",
    "rnn.add(LSTM(units=50, return_sequences=True))\n",
    "rnn.add(Dropout(0.2))\n",
    "# Third LSTM layer\n",
    "rnn.add(LSTM(units=50, return_sequences=True))\n",
    "rnn.add(Dropout(0.2))\n",
    "# Fourth LSTM layer\n",
    "rnn.add(LSTM(units=50))\n",
    "rnn.add(Dropout(0.2))\n",
    "# The output layer\n",
    "rnn.add(Dense(units=1))\n",
    "# Adding The Output Layer To Our Recurrent Neural Network\n",
    "rnn.add(Dense(units=1))\n",
    "\n",
    "# Compiling the RNN\n",
    "# the compilation step of building a neural network is where we specify \n",
    "# the neural netâ€™s optimizer and loss function\n",
    "#rnn.compile(optimizer='rmsprop',loss='mean_squared_error') \n",
    "rnn.compile(optimizer='adam',loss='mean_squared_error', metrics=['accuracy']) \n",
    "# The Adam optimizer is a workhorse optimizer that is useful in a wide variety of neural network architectures.\n",
    "# Fitting the recurrent neural network on the training set\n",
    "rnn.fit(X_train_data,y_train_data,epochs=50,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a9cc0",
   "metadata": {},
   "source": [
    "#### Building The Test Data Set\n",
    "Create an array to hold stock prices from Jan 2017 and the 60 trading days prior to January 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "# Used the first 60 entires of test set\n",
    "dataset_total = pd.concat((dataset[\"Close\"][:'2016'],dataset[\"Close\"]['2017':]),axis=0)\n",
    "inputs = dataset_total[len(dataset_total)-len(test_set) - 60:].values\n",
    "inputs = inputs.reshape(-1,1) # To make it suitable for predict method\n",
    "inputs  = sc.transform(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d003133",
   "metadata": {},
   "source": [
    "#### Making Predictions\n",
    "Used the test data set to make predictions by calling the predict method on the rnn object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fde300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the Test Data\n",
    "X_test = []\n",
    "for i in range(60,311):\n",
    "    X_test.append(inputs[i-60:i,0])\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "# making predictions\n",
    "predicted_stock_price = rnn.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "\n",
    "#print(predicted_stock_price.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83969301",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47649a4e",
   "metadata": {},
   "source": [
    "#### Plot the Predictions\n",
    "The plot compares the predicted stock prices with IBM's actual stock price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca137ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set, color='red',label='Actual Stock Price')\n",
    "plt.plot(predicted_stock_price, color='green',label='Predicted Stock Price')\n",
    "plt.title('IBM Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('IBM Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67deb4e4",
   "metadata": {},
   "source": [
    "#### Evaluating the Model\n",
    "Root Mean Square Error (RMSE) is a standard way to measure the error of a model in predicting quantitative data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf32dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = math.sqrt(mean_squared_error(test_set, predicted_stock_price))\n",
    "print(\"The root mean squared error is {}.\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8969f6",
   "metadata": {},
   "source": [
    "The lower the RMSE, the better a given model is able to \"fit\" a dataset\n",
    "\n",
    "With a rmse of 2.4, the model is doing well.\n",
    "\n",
    "### 2. Gated Recurrent Units (GRU)\n",
    "GRU is similar to LSTM, but it has fewer gates. Also, it relies solely on a hidden state for memory transfer between recurrent units, so there is no separate cell state.\n",
    "#### Adding Layers to GRU RNN \n",
    "The GRU RNN is a Sequential Keras model. After initializing the Sequential model, there's need to add in the layers. The first layer to add is the Gated Recurrent Unit layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3169eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GRU architecture\n",
    "regressorGRU = Sequential()\n",
    "# First GRU layer with Dropout regularisation\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train_data.shape[1],1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# Second GRU layer\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train_data.shape[1],1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# Third GRU layer\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train_data.shape[1],1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# Fourth GRU layer\n",
    "regressorGRU.add(GRU(units=50, activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# The output layer\n",
    "regressorGRU.add(Dense(units=1))\n",
    "# Compiling the RNN\n",
    "regressorGRU.compile(optimizer=SGD(learning_rate=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error',metrics=['accuracy'])\n",
    "# Fitting to the training set\n",
    "regressorGRU.fit(X_train_data,y_train_data,epochs=50,batch_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76807da",
   "metadata": {},
   "source": [
    "Used the same dataset to train the GRU model. To train the model in Keras, we just call the fit function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa77033b",
   "metadata": {},
   "source": [
    "#### Test the GRU Model\n",
    "Used the same test data set that was used to predict in the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339bcded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing X_test and predicting the prices\n",
    "X_test = []\n",
    "for i in range(60,311):\n",
    "    X_test.append(inputs[i-60:i,0])\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "GRU_predicted_stock_price = regressorGRU.predict(X_test)\n",
    "GRU_predicted_stock_price = sc.inverse_transform(GRU_predicted_stock_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b3579",
   "metadata": {},
   "source": [
    "#### Predictions Visualization\n",
    "In this case we visualize the predicted values and the actual stock pricce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c055f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set, color='red',label='Actual Stock Price')\n",
    "plt.plot(GRU_predicted_stock_price, color='green',label='Predicted Stock Price')\n",
    "plt.title('IBM Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('IBM Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee93ca",
   "metadata": {},
   "source": [
    "#### Evaluating the Model\n",
    "Root Mean Square Error (RMSE) is a standard way to measure the error of a model in predicting quantitative data.\n",
    "\n",
    "The lower the RMSE, the better a given model is able to \"fit\" a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = math.sqrt(mean_squared_error(test_set, GRU_predicted_stock_price))\n",
    "print(\"The root mean squared error is {}.\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3db30d",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "From the visualization above, after 50 epochs, the model does quite well for both the training and validation data. It predicts the pattern correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "# open a file where to store the data\n",
    "#file=open('lstm_gru_model.pkl','wb')\n",
    "\n",
    "# dump information to that file\n",
    "#pickle.dump(rnn, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model to compare results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
